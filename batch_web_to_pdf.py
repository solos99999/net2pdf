#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç½‘é¡µè½¬PDFå·¥å…·
ä»ç½‘é¡µä¸­è·å–é“¾æ¥ï¼Œé€‰æ‹©éœ€è¦çš„é“¾æ¥ï¼Œæ‰¹é‡è½¬æ¢ä¸ºPDFæ–‡ä»¶
"""

import requests
import os
import sys
import re
from urllib.parse import urlparse, urljoin
from datetime import datetime
import logging
from bs4 import BeautifulSoup
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    import pdfkit
    PDFKIT_AVAILABLE = True
except ImportError:
    PDFKIT_AVAILABLE = False
    logger.warning("pdfkitæœªå®‰è£…ï¼Œå°†ä½¿ç”¨HTMLæ–‡ä»¶ä¿å­˜æ–¹å¼")

class BatchWebToPDF:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        if PDFKIT_AVAILABLE:
            self.pdfkit_options = {
                'page-size': 'A4',
                'margin-top': '0.75in',
                'margin-right': '0.75in',
                'margin-bottom': '0.75in',
                'margin-left': '0.75in',
                'encoding': "UTF-8",
                'no-outline': None,
                'enable-local-file-access': None,
                'disable-smart-shrinking': None,
                'no-stop-slow-scripts': None,
                'load-error-handling': 'ignore',
                'load-media-error-handling': 'ignore',
                'javascript-delay': '1000',
                'no-images': None,
                'disable-javascript': None
            }
    
    def get_webpage_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
            
            if response.encoding.lower() not in ['utf-8', 'utf8']:
                logger.info(f"æ£€æµ‹åˆ°ç¼–ç : {response.encoding}ï¼Œè½¬æ¢ä¸ºUTF-8")
                content = response.content.decode(response.encoding, errors='replace')
                content = content.encode('utf-8').decode('utf-8')
            else:
                content = response.text
            
            return content, response.url
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–ç½‘é¡µå¤±è´¥: {e}")
            raise
    
    def extract_links_from_page(self, url, html_content):
        """ä»ç½‘é¡µä¸­æå–é“¾æ¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                absolute_url = urljoin(url, href)
                
                if self.is_valid_link(absolute_url, text):
                    links.append({
                        'url': absolute_url,
                        'text': text[:50] + '...' if len(text) > 50 else text,
                        'title': link.get('title', '')
                    })
            
            # å»é‡
            unique_links = []
            seen_urls = set()
            for link in links:
                if link['url'] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link['url'])
            
            logger.info(f"ä»é¡µé¢ä¸­æå–åˆ° {len(unique_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            return unique_links
            
        except Exception as e:
            logger.error(f"æå–é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def is_valid_link(self, url, text):
        """åˆ¤æ–­é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
        invalid_patterns = [
            r'javascript:',
            r'mailto:',
            r'tel:',
            r'#',
            r'\.(css|js|png|jpg|jpeg|gif|ico|pdf|zip|rar)$',
            r'logout',
            r'login',
            r'admin',
            r'\.(com|cn|org|net)/$'
        ]
        
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        if not text.strip():
            return False
        
        return True
    
    def display_links(self, links):
        """æ˜¾ç¤ºé“¾æ¥åˆ—è¡¨ä¾›ç”¨æˆ·é€‰æ‹©"""
        print(f"\nå‘ç° {len(links)} ä¸ªé“¾æ¥:")
        print("-" * 80)
        
        for i, link in enumerate(links, 1):
            print(f"{i:2d}. {link['text']}")
            print(f"    URL: {link['url']}")
            if link['title']:
                print(f"    æ ‡é¢˜: {link['title']}")
            print()
        
        return self.get_user_selection(links)
    
    def get_user_selection(self, links):
        """è·å–ç”¨æˆ·é€‰æ‹©çš„é“¾æ¥"""
        while True:
            try:
                print("è¯·é€‰æ‹©è¦è½¬æ¢çš„é“¾æ¥ (è¾“å…¥æ•°å­—ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œè¾“å…¥ 'all' é€‰æ‹©å…¨éƒ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º):")
                choice = input("æ‚¨çš„é€‰æ‹©: ").strip()
                
                if choice.lower() == 'quit':
                    return []
                elif choice.lower() == 'all':
                    return [link['url'] for link in links]
                else:
                    indices = [int(x.strip()) - 1 for x in choice.split(',')]
                    selected_links = []
                    
                    for idx in indices:
                        if 0 <= idx < len(links):
                            selected_links.append(links[idx]['url'])
                        else:
                            print(f"æ— æ•ˆçš„é€‰æ‹©: {idx + 1}")
                    
                    if selected_links:
                        return selected_links
                    else:
                        print("è¯·é€‰æ‹©æœ‰æ•ˆçš„é“¾æ¥")
                        
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            except KeyboardInterrupt:
                print("\næ“ä½œè¢«å–æ¶ˆ")
                return []
    
    def convert_url_to_pdf(self, url, output_dir="batch_outputs"):
        """è½¬æ¢å•ä¸ªURLä¸ºPDF"""
        try:
            html_content, final_url = self.get_webpage_content(url)
            
            if PDFKIT_AVAILABLE:
                try:
                    output_path = self.generate_filename(final_url, output_dir, "pdf")
                    self.convert_html_to_pdf(html_content, output_path)
                    return output_path, "pdf"
                except Exception as e:
                    logger.warning(f"HTMLè½¬PDFå¤±è´¥ï¼Œå°†ä¿å­˜ä¸ºHTML: {e}")
            
            output_path = self.generate_filename(final_url, output_dir, "html")
            self.save_as_html(html_content, output_path)
            return output_path, "html"
            
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {e}")
            raise
    
    def generate_filename(self, url, output_dir="batch_outputs", extension="pdf"):
        """æ ¹æ®URLç”Ÿæˆæ–‡ä»¶å"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
        
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace('.', '_')
        path = parsed_url.path.strip('/').replace('/', '_') or 'index'
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{domain}_{path}_{timestamp}.{extension}"
        
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        
        return os.path.join(output_dir, filename)
    
    def save_as_html(self, html_content, output_path):
        """ä¿å­˜ä¸ºHTMLæ–‡ä»¶"""
        try:
            enhanced_content = self.enhance_html_for_chinese(html_content)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(enhanced_content)
            logger.info(f"HTMLæ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_path}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def enhance_html_for_chinese(self, html_content):
        """å¢å¼ºHTMLå†…å®¹çš„ä¸­æ–‡æ”¯æŒ"""
        if '<meta charset=' not in html_content and '<meta http-equiv="Content-Type"' not in html_content:
            if '<head>' in html_content:
                html_content = html_content.replace('<head>', '<head>\n    <meta charset="UTF-8">')
            elif '<head ' in html_content:
                html_content = re.sub(r'(<head[^>]*>)', r'\1\n    <meta charset="UTF-8">', html_content)
            else:
                html_content = html_content.replace('<html>', '<html>\n<head>\n    <meta charset="UTF-8">\n</head>')
        
        font_css = """
        <style>
        body {
            font-family: "Microsoft YaHei", "SimSun", "SimHei", "KaiTi", "FangSong", Arial, sans-serif;
            line-height: 1.6;
        }
        </style>
        """
        
        if '<head>' in html_content:
            html_content = html_content.replace('<head>', '<head>' + font_css)
        elif '<head ' in html_content:
            html_content = re.sub(r'(<head[^>]*>)', r'\1' + font_css, html_content)
        
        return html_content
    
    def convert_html_to_pdf(self, html_content, output_path):
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºPDF"""
        try:
            logger.info(f"æ­£åœ¨å°†HTMLå†…å®¹è½¬æ¢ä¸ºPDF: {output_path}")
            
            temp_html = "temp_" + datetime.now().strftime("%Y%m%d_%H%M%S") + ".html"
            with open(temp_html, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            try:
                config = pdfkit.configuration(wkhtmltopdf=r'C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe')
                pdfkit.from_file(temp_html, output_path, options=self.pdfkit_options, configuration=config)
                logger.info(f"PDFæ–‡ä»¶ç”ŸæˆæˆåŠŸ: {output_path}")
                return True
            finally:
                if os.path.exists(temp_html):
                    os.remove(temp_html)
                    
        except Exception as e:
            logger.error(f"HTMLè½¬PDFå¤±è´¥: {e}")
            raise
    
    def batch_convert(self, urls, output_dir="batch_outputs"):
        """æ‰¹é‡è½¬æ¢URLåˆ—è¡¨"""
        results = []
        total = len(urls)
        
        print(f"\nå¼€å§‹æ‰¹é‡è½¬æ¢ {total} ä¸ªé“¾æ¥...")
        print("=" * 60)
        
        for i, url in enumerate(urls, 1):
            try:
                print(f"\n[{i}/{total}] æ­£åœ¨è½¬æ¢: {url}")
                
                output_path, file_type = self.convert_url_to_pdf(url, output_dir)
                
                file_size = os.path.getsize(output_path) / 1024
                print(f"âœ… æˆåŠŸ: {output_path} ({file_type.upper()}, {file_size:.2f} KB)")
                
                results.append({
                    'url': url,
                    'output_path': output_path,
                    'file_type': file_type,
                    'file_size': file_size,
                    'status': 'success'
                })
                
                if i < total:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"âŒ å¤±è´¥: {e}")
                results.append({
                    'url': url,
                    'error': str(e),
                    'status': 'failed'
                })
        
        return results
    
    def process_main_page(self, url):
        """å¤„ç†ä¸»é¡µé¢ï¼Œæå–é“¾æ¥å¹¶æ‰¹é‡è½¬æ¢"""
        try:
            print(f"æ­£åœ¨åˆ†æä¸»é¡µé¢: {url}")
            
            html_content, final_url = self.get_webpage_content(url)
            links = self.extract_links_from_page(final_url, html_content)
            
            if not links:
                print("æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥")
                return
            
            selected_urls = self.display_links(links)
            
            if not selected_urls:
                print("æœªé€‰æ‹©ä»»ä½•é“¾æ¥")
                return
            
            results = self.batch_convert(selected_urls)
            self.show_batch_results(results)
            
        except Exception as e:
            logger.error(f"å¤„ç†ä¸»é¡µé¢å¤±è´¥: {e}")
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")

    def show_batch_results(self, results):
        """æ˜¾ç¤ºæ‰¹é‡è½¬æ¢ç»“æœ"""
        print("\n" + "=" * 60)
        print("æ‰¹é‡è½¬æ¢ç»“æœç»Ÿè®¡")
        print("=" * 60)
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        failed_count = len(results) - success_count
        
        print(f"æ€»é“¾æ¥æ•°: {len(results)}")
        print(f"æˆåŠŸè½¬æ¢: {success_count}")
        print(f"è½¬æ¢å¤±è´¥: {failed_count}")
        
        if success_count > 0:
            total_size = sum(r['file_size'] for r in results if r['status'] == 'success')
            print(f"æ€»æ–‡ä»¶å¤§å°: {total_size:.2f} KB")
            
            pdf_count = sum(1 for r in results if r['status'] == 'success' and r['file_type'] == 'pdf')
            html_count = success_count - pdf_count
            
            print(f"PDFæ–‡ä»¶: {pdf_count}")
            print(f"HTMLæ–‡ä»¶: {html_count}")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(results, 1):
            if result['status'] == 'success':
                print(f"{i}. âœ… {result['url']} -> {result['output_path']}")
            else:
                print(f"{i}. âŒ {result['url']} -> {result['error']}")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("=" * 60)
    print("æ‰¹é‡ç½‘é¡µè½¬PDFå·¥å…·")
    print("=" * 60)
    
    if not PDFKIT_AVAILABLE:
        print("âš ï¸  æ³¨æ„: pdfkitæœªå®‰è£…ï¼Œå°†ä¿å­˜ä¸ºHTMLæ–‡ä»¶")
        print("   è¦ç”ŸæˆPDFï¼Œè¯·å®‰è£…: pip install pdfkit")
        print("   å¹¶ä¸‹è½½wkhtmltopdf: https://wkhtmltopdf.org/downloads.html")
        print()
    else:
        print("âœ… PDFç”ŸæˆåŠŸèƒ½å·²å¯ç”¨")
        print("ğŸ”¤ ä¸­æ–‡æ”¯æŒå·²ä¼˜åŒ–")
        print()
    
    converter = BatchWebToPDF()
    
    while True:
        try:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. è¾“å…¥ç½‘é¡µURLï¼Œæå–é“¾æ¥å¹¶æ‰¹é‡è½¬æ¢")
            print("2. ç›´æ¥è¾“å…¥å¤šä¸ªURLè¿›è¡Œæ‰¹é‡è½¬æ¢")
            print("3. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
            
            if choice == '1':
                url = input("\nè¯·è¾“å…¥ç½‘é¡µURL: ").strip()
                if not url:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
                    continue
                
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                
                converter.process_main_page(url)
                
            elif choice == '2':
                print("\nè¯·è¾“å…¥å¤šä¸ªURL (æ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):")
                urls = []
                while True:
                    url = input().strip()
                    if not url:
                        break
                    if not url.startswith(('http://', 'https://')):
                        url = 'https://' + url
                    urls.append(url)
                
                if urls:
                    results = converter.batch_convert(urls)
                    converter.show_batch_results(results)
                else:
                    print("æœªè¾“å…¥ä»»ä½•URL")
                    
            elif choice == '3':
                print("ç¨‹åºé€€å‡º")
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
            print("è¯·ç¨åé‡è¯•")

if __name__ == "__main__":
    main() 